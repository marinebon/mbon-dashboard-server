# Airflow sections based on provided example docker-compose.yml included in
# airflow's docs here:
# https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml
---

# ============================================================================
# === config shared between containers =======================================
# ============================================================================
x-global-common:
  &global-common
  environment:
    &global-common-env
    INFLUXDB_HOSTNAME: http://influxdb:8086
    INFLUXDB_TOKEN: ${INFLUXDB_TOKEN}
# ============================================================================
# ============================================================================
# === config shared between all airflow services =============================
# ============================================================================
x-airflow-common:
  &airflow-common
  <<: *global-common
  environment:
    &airflow-common-env
    <<: *global-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow  # TODO: rm this bc it is deprecated
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__PARALLELISM: '50'
    AIRFLOW__SCHEDULER__MAX_THREADS: '8'
    # per-DAG max concurrent tasks (core.dag_concurrency)
    AIRFLOW__CORE__DAG_CONCURRENCY: '20'
      
    # per-DAG max active tasks (core.max_active_tasks_per_dag)
    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: '20'

    # per-DAG max active runs (core.max_active_runs_per_dag)
    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: '5'  

    # how many tasks a Celery worker will execute in parallel
    AIRFLOW__CELERY__WORKER_CONCURRENCY: '16'

    # TODO: uploader hostname no longer used? rm?
    UPLOADER_HOSTNAME: http://mbon_data_uploader:5000
    SQLALCHEMY_WARN_20: 0  # set to 1 verbose logging about sqlAlchemy deprecations
    AIRFLOW__LOGGING__LOGGING_LEVEL: INFO  # DEBUG
    AIRFLOW_HOME: /opt/airflow/
    _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}  # un for admin login
    _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}  # pw for admin login
    # metadata for that user
    _AIRFLOW_WWW_USER_FIRSTNAME: Tylar
    _AIRFLOW_WWW_USER_LASTNAME: Murray
    _AIRFLOW_WWW_USER_ROLE: Admin
    _AIRFLOW_WWW_USER_EMAIL: muray.tylar@gmail.com
    AIRFLOW__WEBSERVER__BASE_URL: https://mbon-dashboards.marine.usf.edu:8080
    AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: "True"
    FORWARDED_ALLOW_IPS: "*"
    # <-- tell ProxyFix how many X-Forwarded-* values to trust
    AIRFLOW__WEBSERVER__PROXY_FIX_X_FOR:
    AIRFLOW__WEBSERVER__PROXY_FIX_X_PROTO:
    AIRFLOW__WEBSERVER__PROXY_FIX_X_HOST:
    AIRFLOW__WEBSERVER__PROXY_FIX_X_PORT:
    AIRFLOW__WEBSERVER__PROXY_FIX_X_PREFIX: 1
    AIRFLOW__WEBSERVER__WEB_SERVER_HOST: 0.0.0.0
    AIRFLOW__WEBSERVER__WEB_SERVER_PORT: 8080
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - airflow-logs-volume:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
#     - /srv/imars-objects:/srv/imars-objects:ro,slave
#     - ./airflow/requirements.txt:/requirements.txt
  user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}"  # user & group ID in airflow containers
  depends_on:
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy
  build:
    context: ./airflow
    args:
      INFLUXDB_HOSTNAME: http://influxdb:8086
      INFLUXDB_TOKEN: ${INFLUXDB_TOKEN}

# ============================================================================

services:
  nginx:
    container_name: nginx
    build: ./nginx
    restart: unless-stopped
    ports:
      - 80:80
      - 443:443
      - 8080:8080
      - 8086:8086
      - 5555:5555
    volumes:
      - ./static_files:/usr/share/nginx/html:ro
      - ./certs:/etc/nginx/certs
    env_file:
      - .env
    depends_on:
      grafana:
        condition: service_healthy

  # ==========================================================================
  # === grafana dashboard services
  # ==========================================================================
  grafana:
      <<: *global-common
      container_name: grafana
      image: grafana/grafana:10.4.4
      ports:
          - 3000:3000
      environment:
          <<: *global-common-env
          GF_INSTALL_PLUGINS: "alexandra-trackmap-panel,pierosavi-imageit-panel,fatcloud-windrose-panel,https://github.com/USF-IMARS/grafana-erddap/releases/download/v2.0.5/imars-grafanaerddap-panel-2.0.5.zip;imars-grafanaerddap-panel,yesoreyeram-infinity-datasource"
          GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS: "imars-grafanaerddap-panel"
          GF_PATHS_PROVISIONING: /grafana_provisioning
          GF_AUTH_ANONYMOUS_ENABLED: true
          GF_SECURITY_ADMIN_USER: imars_grafana_user
          GF_SECURITY_ADMIN_PASSWORD: ${GF_SECURITY_ADMIN_PASSWORD}
          INFLUXDB_GRAPHITE_HOSTNAME: http://influxdb:2003
          GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH: /grafana_provisioning/dashboards/home_dashboard.json
          #GF_SERVER_ROOT_URL: http://fknms.mbon-dashboards.marine.usf.edu/
          #GF_SERVER_SERVER_FROM_SUB_PATH: true
          
      volumes:
          - ./grafana/provisioning:/grafana_provisioning
          - grafana-storage:/var/lib/grafana
          - ./grafana/public/maps:/usr/share/grafana/public/maps
      env_file:
          - .env
      healthcheck:
        test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
        interval: 10s
        timeout: 5s
        retries: 12
      
  influxdb:
        container_name: influxdb
        image: influxdb:2.7.6
        ports:
            - 2003:2003  # graphite API  TODO: rm this or mv it to nginx like others
        volumes:
            # init the db by putting .sh or .iql files in /docker-entrypoint-initdb.d
            # this should inlcude adding a read-only user for grafana
            - ./influxdb/initdb:/docker-entrypoint-initdb.d
            # docker data volume:
            - influx-data-volume:/var/lib/influxdb2
        restart: always
        # --- Memory guardrails for rare spikes ---
        mem_reservation: 16g       # soft threshold
        mem_limit: 20g             # hard ceiling
        memswap_limit: 24g         # hard + swap allowance (~4 GiB swap)

        # Optional safety valves
        pids_limit: 4096
        ulimits:
          nofile:
            soft: 65535
            hard: 65535

        # Healthcheck (causes restart if unhealthy)
        healthcheck:
          test: ["CMD", "wget", "-qO-", "http://127.0.0.1:8086/health"]
          interval: 30s
          timeout: 5s
          retries: 5

        environment:
            - INFLUXDB_DB=fwc_coral_disease
            - INFLUXDB_GRAPHITE_ENABLED=true
            - INFLUXDB_HTTP_AUTH_ENABLED=false  # TODO: make this true & + users.

            # === logging settings
            - INFLUXDB_LOGGING_LEVEL=warn
            - INFLUXDB_META_LOGGING_ENABLED=false
            # - INFLUXDB_DATA_TRACE_LOGGING_ENABLED=false
            - INFLUXDB_DATA_QUERY_LOG_ENABLED=false
            - INFLUXDB_HTTP_LOG_ENABLED=false
            - INFLUXDB_CONTINUOUS_QUERIES_LOG_ENABLED=false

            # === user setup
            - DOCKER_INFLUXDB_INIT_MODE=setup
            - DOCKER_INFLUXDB_INIT_USERNAME=imars_influx_user
            - DOCKER_INFLUXDB_INIT_PASSWORD=${INFLUX_PASSWORD}
            - DOCKER_INFLUXDB_INIT_ORG=imars
            - DOCKER_INFLUXDB_INIT_BUCKET=imars_bucket
            # - DOCKER_INFLUXDB_INIT_RETENTION=1w
            - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=${INFLUX_ADMIN_TOKEN} 
            # - INFLUXDB_USER=influx_user         #       See https://github.com/influxdata/influxdata-docker/issues/232
            # - INFLUXDB_ADMIN_USER=influx_admin  #       and https://github.com/influxdata/influxdata-docker/issues/224
            # - INFLUXDB_READ_USER=grafana
            # - INFLUXDB_WRITE_USER=telegraf
	          # Storage / TSI cache guardrails

            # === RESOURCE LIMITATIONS TO PREVENT HARD CRASHES
            - INFLUXD_STORAGE_CACHE_MAX_MEMORY_SIZE=1GiB
            # If available in your build, this helps avoid spikes with huge series sets:
            - INFLUXD_STORAGE_SERIES_ID_SET_CACHE_SIZE=512MiB

            # Query concurrency (avoid stampedes)
            - INFLUXD_QUERY_CONCURRENCY=4
            # If exposed in your build, these further cap query spikes (safe defaults shown):
            - INFLUXD_QUERY_MAX_MEMORY_BYTES=2147483648   # 2 GiB
            - INFLUXD_QUERY_QUEUE_SIZE=128

            # Reasonable server timeouts (optional)
            - INFLUXD_HTTP_READ_TIMEOUT=30s
            - INFLUXD_HTTP_WRITE_TIMEOUT=30s

        env_file:
            - .env
  # ==========================================================================
  # ==========================================================================
  # === airflow job orchestration services
  # ==========================================================================
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow  # ${POSTGRES_PASSWORD}
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always
    env_file:
      - .env
      
  redis:
    image: redis:latest
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always
    env_file:
      - .env
      
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    environment:
      <<: *airflow-common-env
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    # container_name: airflow_webserver
    # mem_limit: 2048m
    env_file:
      - .env
      
  airflow-scheduler:
    <<: *airflow-common
    environment:
      <<: *airflow-common-env
    command: scheduler
    restart: always
    env_file:
      - .env
      
  airflow-worker:
    <<: *airflow-common
    environment:
      <<: *airflow-common-env
      AIRFLOW__CORE__PARALLELISM: '50'
    command: celery worker
    restart: always
    env_file:
      - .env
      
  airflow-init:
    <<: *airflow-common
    command: version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
    env_file:
      - .env
      
  flower:
    <<: *airflow-common
    command: celery flower
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    env_file:
      - .env
  # ==========================================================================

volumes:
  postgres-db-volume:
  influx-data-volume:
  grafana-storage:
  airflow-logs-volume:
